[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NanoClass2",
    "section": "",
    "text": "NanoClass2 is a taxonomic meta-classifier for 16S/18S amplicon sequencing data generated with the Oxford Nanopore MinION. The first iteration of this workflow, NanoClass, was originally developed by Evelien Jongepier and the original version can be found here.\nWith a single command, you can run upto 11 classification tools on multiple samples in parallel, including BLASTN, Centrifuge, Kraken2, IDTAXA, MegaBLAST, dcMegaBLAST, Minimap2, Mothur, QIIME2, RDP and SPINGO. Read preparation steps, such as quality trimming, length filtering and sub-sampling, are an integral part of the pipeline.\nNanoClass2 automatically installs all software packages and dependencies, downloads and builds required taxonomic databases and runs the analysis on your samples.\n\n\nThe following changes were made compared to the first version of NanoClass:\n\nUse of SILVA 138.1_SSURef_NR99 instead pf SILVA 132_SSURef_NR99\nUpdating to Silva 138 now also means eukaryotic sequences are included in the database\nUpdated Kraken2 to work with the changed SILVA ftp path\nRead filtering is now done with Chopper instead of Nanofilt\nChopper allows additional quality filtering and to trim nucleotides from the beginning and the end of the read\nAll classifiers use now exactly the same reference files\nUpdated the output folder structure\nAdded additional parameters for kraken2 and minimap2\nClarified version numbers throughout the conda environment files",
    "crumbs": [
      "NanoClass2"
    ]
  },
  {
    "objectID": "index.html#nanoclass2",
    "href": "index.html#nanoclass2",
    "title": "NanoClass2",
    "section": "NanoClass2",
    "text": "NanoClass2\nNanoClass2 is a taxonomic meta-classifier for 16S/18S amplicon sequencing data generated with the Oxford Nanopore MinION. The first iteration of this workflow, NanoClass, was originally developed by Evelien Jongepier and the original version can be found here.\nWith a single command, you can run up to 11 classification tools on multiple samples in parallel, including BLASTN, Centrifuge, Kraken2, IDTAXA, MegaBLAST, dcMegaBLAST, Minimap2, Mothur, QIIME2, RDP and SPINGO. Read preparation steps, such as quality trimming, length filtering and sub-sampling, are an integral part of the pipeline.\nNanoClass2 automatically installs all software packages and dependencies, downloads and builds required taxonomic databases and runs the analysis on your samples.\n\nChanges\nThe following changes were made compared to the first version of NanoClass:\n\nUse of SILVA 138.1_SSURef_NR99 instead pf SILVA 132_SSURef_NR99\nUpdating to Silva 138 now also means eukaryotic sequences are included in the database\nUpdated Kraken2 to work with the changed SILVA ftp path\nRead filtering is now done with Chopper instead of Nanofilt\nChopper allows additional quality filtering and to trim nucleotides from the beginning and the end of the read\nAll classifiers use now exactly the same reference files\nUpdated the output folder structure\nAdded additional parameters for kraken2 and minimap2\nClarified version numbers throughout the conda environment files",
    "crumbs": [
      "NanoClass2"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "NanoITS",
    "section": "Quick start",
    "text": "Quick start\n…"
  },
  {
    "objectID": "source/code_dev.html",
    "href": "source/code_dev.html",
    "title": "NanoClass2",
    "section": "",
    "text": "Vroom version: vroom_1.5.7\nkraken2 –db /home/ndombro/personal/testing/NanoClass2/./db/common –confidence 0.0 –output classifications/demo/kraken/R1A.kraken.out –report classifications/demo/kraken/R1A.kraken.report –gzip-compressed –threads 16 data/demo/chopper/R1A.subsampled.fastq.gz\nGithub testrun:\n\ncd /home/ndombro/personal/testing\n\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\nmkdir analyses\ncd analyses\n\ncp ~/personal/testing/NanoClass2/config.yaml .\ncp ~/personal/testing/NanoClass2/jobscript.sh .\n\nsnakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix /home/ndombro/personal/testing/NanoClass2/.snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\nsnakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --report\n\nChanges made to config.yaml:\n\nsamples: “../NanoClass2/example_files/mapping.csv”\n\nChanges to mapping.csv\n\nChange path to: /home/ndombro/personal/testing/NanoClass2/example_files/barcode01_merged.fastq.gz\n\n/home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -k classifications/demo/kraken/R3A.kraken.out -f /home/ndombro/personal/testing/NanoClass2/./db/kraken/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta -m /home/ndombro/personal/testing/NanoClass2/./db/kraken/seqid2taxid.map\n\n\n\n\nCentrifuge has a problem that the counts are larger than 100. To solve this, we set the -k to 1 for now. For later\n\ncd results/classifications/demo/centrifuge\n\n#check why we do not max out at 100\nfor file in */*otumat; do\n    sum=$(awk -F'\\t' 'NR&gt;1 {sum+=$2} END {print sum}' \"$file\")\n    echo \"Sum of Column 2 in $file: $sum\"\ndone\n\n#check unique labels in the raw reads (are 100)\nfor file in *out; do\n    count=$(awk  -F'\\t' '{print $1}' \"$file\" | sort | uniq | wc -l)\n    echo \"read number in $file: $count\"\ndone\n\n#check total counts\nwc -l *out\n\n#check unique genera in the raw reads (are 100)\nfor file in *out; do\n    count=$(awk -F'\\t' '{print $2}' \"$file\" | sort | uniq | wc -l)\n    echo \"read number in $file: $count\"\ndone\n\n\n#get a new ref file\npython add_taxonomy.py -i ref-seqs.fna -t ref-taxonomy.txt -o ref-seqs-tax.fna\n\n\n\nconda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/e3443d8e60db4c8046194cb2448abc78\n\npython3 /home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R1A.centrifuge.out -f /home/ndombro/personal/testing/NanoClass2/db/common/ref-seqs-tax.fna\n\nSum of Column 2 in centrifuge/R1A.centrifuge.otumat: 84 Sum of Column 2 in centrifuge/R2A.centrifuge.otumat: 74 Sum of Column 2 in centrifuge/R3A.centrifuge.otumat: 110\nProblem with R:\nclassifications/demo/kraken/R1A.kraken.taxmat, classifications/demo/kraken/R2A.kraken.taxmat, classifications/demo/kraken/R3A.kraken.taxmat\n\nconda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/fbab651501bcaffe2471e33eec5a7215\n\nRscript /home/ndombro/personal/testing/NanoClass2//scripts/barplot.R classifications/demo/kraken/R1A.kraken.taxmat classifications/demo/kraken/R2A.kraken.taxmat classifications/demo/kraken/R3A.kraken.taxmat\n\nReturns :\nRows: 152 Columns: 7 ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: “ chr (7): #taxid, Domain, Phylum, Class, Order, Family, Genus\nℹ Use spec() to retrieve the full column specification for this data. ℹ Specify the column types or set show_col_types = FALSE to quiet this message.\n\nsuppressPackageStartupMessages(library(phyloseq))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(vroom))\nsuppressPackageStartupMessages(library(tidyr))\n\nargs &lt;- c(\"classifications/demo/kraken/R1A.kraken.taxmat\",\n                \"classifications/demo/kraken/R2A.kraken.taxmat\", \n                \"classifications/demo/kraken/R3A.kraken.taxmat\")\n\n\ntaxmat &lt;- as.data.frame(unique(vroom(delim = '\\t', args)))\ntaxmat &lt;- taxmat[!is.na(taxmat$Domain),]\n\ntaxmat &lt;- unique(taxmat)\nnames(taxmat)[1] &lt;- \"taxid\"\n#$write.table(taxmat, file=\"tables/taxonomy-table.tsv\", row.names=F, col.names=T, sep='\\t', quote=F)\n\nrownames(taxmat) &lt;- taxmat$taxid\ntaxmat$taxid &lt;- NULL\ntaxmat &lt;- as.matrix(taxmat)\n\nfile &lt;- gsub(\".taxmat$\", \".otumat\", args)\nsam &lt;- data.frame(run = rep(NA, length(file)), sample = rep(NA, length(file)), method = rep(NA, length(file)))\n\nfor (i in 1:length(file)){\n  otumatje &lt;- read.table(file[i], header = T, sep = '\\t', comment = \"\")\n  names(otumatje)[1] &lt;- \"taxid\"\n  names(otumatje)[2] &lt;- paste0(strsplit(file[i], \"/\")[[1]][2],\"_\",names(otumatje)[2])\n  ifelse (i == 1, otumat &lt;- otumatje, otumat &lt;- merge(otumat, otumatje, all=TRUE))\n\n  lab = strsplit(file[i], \"/\")[[1]][4]\n  sam$run[i] = strsplit(file[i], \"/\")[[1]][2]\n  sam$sample[i] = strsplit(lab, \"[.]\")[[1]][1]\n  sam$method[i] = strsplit(lab, \"[.]\")[[1]][2]\n} \n\notumat[is.na(otumat)] &lt;- 0\n\n## for some custom DB like BOLD local copy, the same taxonomic lineage may occur 2x.\n## This causes an error when they are used as rownames, which should be unique --&gt; fix: aggregate\notumat &lt;- aggregate(otumat[2:length(otumat)], by=list(otumat$taxid), sum)\nnames(otumat)[1] &lt;- \"taxid\"\n#write.table(otumat, file=\"tables/otu-table.tsv\", row.names=F, col.names=T, sep='\\t', quote=F)\n\nrownames(otumat) &lt;- otumat$taxid\notumat$taxid &lt;- NULL\notumat &lt;- as.matrix(otumat)\n\nrownames(sam) &lt;- colnames(otumat)\n\n\nOTU = otu_table(otumat, taxa_are_rows = TRUE)\nTAX = tax_table(taxmat)\nSAM = sample_data(sam)\n\nphyseq = phyloseq(OTU, TAX, SAM)\npphyseq  = transform_sample_counts(physeq, function(x) x / sum(x) )\n\ntheme_set(theme_bw())\n\ntop.taxa &lt;- tax_glom(physeq, \"Class\")\nTopNOTUs &lt;- names(sort(taxa_sums(top.taxa), TRUE)[1:20])\nBottumNOTUs &lt;- names(taxa_sums(top.taxa))[which(!names(taxa_sums(top.taxa)) %in% TopNOTUs)]\nmerged_physeq = merge_taxa(top.taxa, BottumNOTUs, 2)\n\nmdf = psmelt(merged_physeq); names(mdf)[names(mdf) == \"Class\"] &lt;- \"level\"\nmdf$OTU[which(is.na(mdf$level))] &lt;- \"aaaOther\"\nmdf$level[which(is.na(mdf$level))] &lt;- \"aaaOther\"\naggr_mdf &lt;- aggregate(Abundance ~ sample + run + method + level, data = mdf, sum)\n\nlabs = aggr_mdf$level; labs[labs==\"aaaOther\"] &lt;- \"Other\"\ncols = scales::hue_pal()(length(unique(labs))); cols[unique(labs) == \"Other\"] &lt;- \"#CCCCCC\"\n\np = ggplot(aggr_mdf, aes_string(x = \"method\", y = \"Abundance\", fill = \"level\"))\np = p + scale_fill_manual(name = \"Phylum\", labels = unique(labs), values = cols)\np = p + facet_grid(paste0(\"\", aggr_mdf$run) ~ paste0(\"\", aggr_mdf$sample))\np = p + geom_bar(stat = \"identity\", position = \"stack\",  color = \"black\", size = 0.1)\np = p + guides(fill=guide_legend(ncol=1))\np = p + labs(x = \"Method\", y = \"Absolute abundance\")\np = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))\np\n\np = ggplot(aggr_mdf, aes_string(x = \"sample\", y = \"Abundance\", fill = \"level\")) \np = p + scale_fill_manual(name = level, labels = unique(labs), values = cols)\np = p + facet_grid(paste0(\"\", aggr_mdf$run) ~ paste0(\"\", aggr_mdf$method))\np = p + geom_bar(stat = \"identity\", position = \"stack\",  color = \"black\", size = 0.1)\np = p + guides(fill=guide_legend(ncol=1))  \np = p + labs(x = \"Sample\", y = \"Absolute abundance\")\np = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))\np\n\nTo do:\n\nMake all classifiers work with new silva db\ncheck cutoffs for:\n\nkraken\nminimap\n\n\nKraken two downloads:\n\nSILVA_138.1_SSURef_NR99_tax_silva.fasta\ntax_slv_ssu_138.1.txt\n\nTested classifiers:\n\nkraken, runs\nminimap, runs\nblastn, runs\ndcmegablast, runs\nmegablast, runs\nrdp, running\nspingo, running\nidtaxa, running\nmothur, needs aln –&gt; change common\nqiime, should work as is\ncentrifuge, change mapurls\n\nhttps://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva_full_align_trunc.fasta.gz\nhttps://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_tax_silva_full_align_trunc.fasta.gz\nError in rule centrifuge_tomat: jobid: 39 output: classifications/demo/centrifuge/R3A.centrifuge.taxlist, classifications/demo/centrifuge/R3A.centrifuge.taxmat, classifications/demo/centrifuge/R3A.centrifuge.otumat log: logs/demo/centrifuge_tomat_R3A.log (check log file(s) for error message) conda-env: /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9 shell: /home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/c$ (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\n\nconda activate /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9\n\n/home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c results/classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/common/ref-seqs.fna \n\nCentrifuge:\ndefault path: https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz default content:\nAC200763 88363 89885 Bacteria;Proteobacteria\nparsed default content:\nAC200763.88363.89885 138073 AC201869.46386.47908 138073\n\n#wget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1\\\".\\\"$2\\\".\\\"$3\\\"\\t\\\"$(NF)}' &gt; test_default.txt\n\nwget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1\".\"$2\".\"$3\"\\t\"$(NF)}' &gt; test_default.txt"
  },
  {
    "objectID": "source/code_dev.html#notes-on-code-development",
    "href": "source/code_dev.html#notes-on-code-development",
    "title": "NanoClass2",
    "section": "",
    "text": "Vroom version: vroom_1.5.7\nkraken2 –db /home/ndombro/personal/testing/NanoClass2/./db/common –confidence 0.0 –output classifications/demo/kraken/R1A.kraken.out –report classifications/demo/kraken/R1A.kraken.report –gzip-compressed –threads 16 data/demo/chopper/R1A.subsampled.fastq.gz\nGithub testrun:\n\ncd /home/ndombro/personal/testing\n\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\nmkdir analyses\ncd analyses\n\ncp ~/personal/testing/NanoClass2/config.yaml .\ncp ~/personal/testing/NanoClass2/jobscript.sh .\n\nsnakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix /home/ndombro/personal/testing/NanoClass2/.snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\nsnakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --report\n\nChanges made to config.yaml:\n\nsamples: “../NanoClass2/example_files/mapping.csv”\n\nChanges to mapping.csv\n\nChange path to: /home/ndombro/personal/testing/NanoClass2/example_files/barcode01_merged.fastq.gz\n\n/home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -k classifications/demo/kraken/R3A.kraken.out -f /home/ndombro/personal/testing/NanoClass2/./db/kraken/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta -m /home/ndombro/personal/testing/NanoClass2/./db/kraken/seqid2taxid.map\n\n\n\n\nCentrifuge has a problem that the counts are larger than 100. To solve this, we set the -k to 1 for now. For later\n\ncd results/classifications/demo/centrifuge\n\n#check why we do not max out at 100\nfor file in */*otumat; do\n    sum=$(awk -F'\\t' 'NR&gt;1 {sum+=$2} END {print sum}' \"$file\")\n    echo \"Sum of Column 2 in $file: $sum\"\ndone\n\n#check unique labels in the raw reads (are 100)\nfor file in *out; do\n    count=$(awk  -F'\\t' '{print $1}' \"$file\" | sort | uniq | wc -l)\n    echo \"read number in $file: $count\"\ndone\n\n#check total counts\nwc -l *out\n\n#check unique genera in the raw reads (are 100)\nfor file in *out; do\n    count=$(awk -F'\\t' '{print $2}' \"$file\" | sort | uniq | wc -l)\n    echo \"read number in $file: $count\"\ndone\n\n\n#get a new ref file\npython add_taxonomy.py -i ref-seqs.fna -t ref-taxonomy.txt -o ref-seqs-tax.fna\n\n\n\nconda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/e3443d8e60db4c8046194cb2448abc78\n\npython3 /home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R1A.centrifuge.out -f /home/ndombro/personal/testing/NanoClass2/db/common/ref-seqs-tax.fna\n\nSum of Column 2 in centrifuge/R1A.centrifuge.otumat: 84 Sum of Column 2 in centrifuge/R2A.centrifuge.otumat: 74 Sum of Column 2 in centrifuge/R3A.centrifuge.otumat: 110\nProblem with R:\nclassifications/demo/kraken/R1A.kraken.taxmat, classifications/demo/kraken/R2A.kraken.taxmat, classifications/demo/kraken/R3A.kraken.taxmat\n\nconda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/fbab651501bcaffe2471e33eec5a7215\n\nRscript /home/ndombro/personal/testing/NanoClass2//scripts/barplot.R classifications/demo/kraken/R1A.kraken.taxmat classifications/demo/kraken/R2A.kraken.taxmat classifications/demo/kraken/R3A.kraken.taxmat\n\nReturns :\nRows: 152 Columns: 7 ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: “ chr (7): #taxid, Domain, Phylum, Class, Order, Family, Genus\nℹ Use spec() to retrieve the full column specification for this data. ℹ Specify the column types or set show_col_types = FALSE to quiet this message.\n\nsuppressPackageStartupMessages(library(phyloseq))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(vroom))\nsuppressPackageStartupMessages(library(tidyr))\n\nargs &lt;- c(\"classifications/demo/kraken/R1A.kraken.taxmat\",\n                \"classifications/demo/kraken/R2A.kraken.taxmat\", \n                \"classifications/demo/kraken/R3A.kraken.taxmat\")\n\n\ntaxmat &lt;- as.data.frame(unique(vroom(delim = '\\t', args)))\ntaxmat &lt;- taxmat[!is.na(taxmat$Domain),]\n\ntaxmat &lt;- unique(taxmat)\nnames(taxmat)[1] &lt;- \"taxid\"\n#$write.table(taxmat, file=\"tables/taxonomy-table.tsv\", row.names=F, col.names=T, sep='\\t', quote=F)\n\nrownames(taxmat) &lt;- taxmat$taxid\ntaxmat$taxid &lt;- NULL\ntaxmat &lt;- as.matrix(taxmat)\n\nfile &lt;- gsub(\".taxmat$\", \".otumat\", args)\nsam &lt;- data.frame(run = rep(NA, length(file)), sample = rep(NA, length(file)), method = rep(NA, length(file)))\n\nfor (i in 1:length(file)){\n  otumatje &lt;- read.table(file[i], header = T, sep = '\\t', comment = \"\")\n  names(otumatje)[1] &lt;- \"taxid\"\n  names(otumatje)[2] &lt;- paste0(strsplit(file[i], \"/\")[[1]][2],\"_\",names(otumatje)[2])\n  ifelse (i == 1, otumat &lt;- otumatje, otumat &lt;- merge(otumat, otumatje, all=TRUE))\n\n  lab = strsplit(file[i], \"/\")[[1]][4]\n  sam$run[i] = strsplit(file[i], \"/\")[[1]][2]\n  sam$sample[i] = strsplit(lab, \"[.]\")[[1]][1]\n  sam$method[i] = strsplit(lab, \"[.]\")[[1]][2]\n} \n\notumat[is.na(otumat)] &lt;- 0\n\n## for some custom DB like BOLD local copy, the same taxonomic lineage may occur 2x.\n## This causes an error when they are used as rownames, which should be unique --&gt; fix: aggregate\notumat &lt;- aggregate(otumat[2:length(otumat)], by=list(otumat$taxid), sum)\nnames(otumat)[1] &lt;- \"taxid\"\n#write.table(otumat, file=\"tables/otu-table.tsv\", row.names=F, col.names=T, sep='\\t', quote=F)\n\nrownames(otumat) &lt;- otumat$taxid\notumat$taxid &lt;- NULL\notumat &lt;- as.matrix(otumat)\n\nrownames(sam) &lt;- colnames(otumat)\n\n\nOTU = otu_table(otumat, taxa_are_rows = TRUE)\nTAX = tax_table(taxmat)\nSAM = sample_data(sam)\n\nphyseq = phyloseq(OTU, TAX, SAM)\npphyseq  = transform_sample_counts(physeq, function(x) x / sum(x) )\n\ntheme_set(theme_bw())\n\ntop.taxa &lt;- tax_glom(physeq, \"Class\")\nTopNOTUs &lt;- names(sort(taxa_sums(top.taxa), TRUE)[1:20])\nBottumNOTUs &lt;- names(taxa_sums(top.taxa))[which(!names(taxa_sums(top.taxa)) %in% TopNOTUs)]\nmerged_physeq = merge_taxa(top.taxa, BottumNOTUs, 2)\n\nmdf = psmelt(merged_physeq); names(mdf)[names(mdf) == \"Class\"] &lt;- \"level\"\nmdf$OTU[which(is.na(mdf$level))] &lt;- \"aaaOther\"\nmdf$level[which(is.na(mdf$level))] &lt;- \"aaaOther\"\naggr_mdf &lt;- aggregate(Abundance ~ sample + run + method + level, data = mdf, sum)\n\nlabs = aggr_mdf$level; labs[labs==\"aaaOther\"] &lt;- \"Other\"\ncols = scales::hue_pal()(length(unique(labs))); cols[unique(labs) == \"Other\"] &lt;- \"#CCCCCC\"\n\np = ggplot(aggr_mdf, aes_string(x = \"method\", y = \"Abundance\", fill = \"level\"))\np = p + scale_fill_manual(name = \"Phylum\", labels = unique(labs), values = cols)\np = p + facet_grid(paste0(\"\", aggr_mdf$run) ~ paste0(\"\", aggr_mdf$sample))\np = p + geom_bar(stat = \"identity\", position = \"stack\",  color = \"black\", size = 0.1)\np = p + guides(fill=guide_legend(ncol=1))\np = p + labs(x = \"Method\", y = \"Absolute abundance\")\np = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))\np\n\np = ggplot(aggr_mdf, aes_string(x = \"sample\", y = \"Abundance\", fill = \"level\")) \np = p + scale_fill_manual(name = level, labels = unique(labs), values = cols)\np = p + facet_grid(paste0(\"\", aggr_mdf$run) ~ paste0(\"\", aggr_mdf$method))\np = p + geom_bar(stat = \"identity\", position = \"stack\",  color = \"black\", size = 0.1)\np = p + guides(fill=guide_legend(ncol=1))  \np = p + labs(x = \"Sample\", y = \"Absolute abundance\")\np = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))\np\n\nTo do:\n\nMake all classifiers work with new silva db\ncheck cutoffs for:\n\nkraken\nminimap\n\n\nKraken two downloads:\n\nSILVA_138.1_SSURef_NR99_tax_silva.fasta\ntax_slv_ssu_138.1.txt\n\nTested classifiers:\n\nkraken, runs\nminimap, runs\nblastn, runs\ndcmegablast, runs\nmegablast, runs\nrdp, running\nspingo, running\nidtaxa, running\nmothur, needs aln –&gt; change common\nqiime, should work as is\ncentrifuge, change mapurls\n\nhttps://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva_full_align_trunc.fasta.gz\nhttps://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_tax_silva_full_align_trunc.fasta.gz\nError in rule centrifuge_tomat: jobid: 39 output: classifications/demo/centrifuge/R3A.centrifuge.taxlist, classifications/demo/centrifuge/R3A.centrifuge.taxmat, classifications/demo/centrifuge/R3A.centrifuge.otumat log: logs/demo/centrifuge_tomat_R3A.log (check log file(s) for error message) conda-env: /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9 shell: /home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/c$ (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\n\nconda activate /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9\n\n/home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c results/classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/common/ref-seqs.fna \n\nCentrifuge:\ndefault path: https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz default content:\nAC200763 88363 89885 Bacteria;Proteobacteria\nparsed default content:\nAC200763.88363.89885 138073 AC201869.46386.47908 138073\n\n#wget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1\\\".\\\"$2\\\".\\\"$3\\\"\\t\\\"$(NF)}' &gt; test_default.txt\n\nwget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1\".\"$2\".\"$3\"\\t\"$(NF)}' &gt; test_default.txt"
  },
  {
    "objectID": "source/installation_instructions.html",
    "href": "source/installation_instructions.html",
    "title": "Installation",
    "section": "",
    "text": "To be able to run this workflow you need to have conda or mamba installed. If you do not have either of these, check out the installation instruction for conda. Since July 2023 conda comes with mamba installed and should be available when updating conda to v23.10.0.\nIf you only want to install mamba, follow the instructions found here.\n\n\n\nThis workflow was developed using snakemake v6.8.0 and python3.9.7 and you might run into problems when using different versions, which is why we recommend installing snakemake as follows:\n\nmamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n\n\n\nNanoClass2 can be installed via git as follows:\n\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\nIf you don’t have or want to install git, you can also download NanoClass as follows:\n\nGo to https://github.com/ndombrowski/NanoClass2\nClick the green code button\nDownload zip\nExtract zip",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "source/installation_instructions.html#install-condamamba",
    "href": "source/installation_instructions.html#install-condamamba",
    "title": "Installation",
    "section": "Install conda/mamba",
    "text": "Install conda/mamba\nTo be able to run this workflow you need to have conda or mamba installed. If you do not have either of these, check out the installation instruction for conda. Since July 2023 conda comes with mamba installed and should be available when updating conda to v23.10.0.\nIf you only want to install mamba, follow the instructions found here.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "source/installation_instructions.html#install-snakemake",
    "href": "source/installation_instructions.html#install-snakemake",
    "title": "Installation",
    "section": "Install snakemake",
    "text": "Install snakemake\nThis workflow was developed using snakemake v6.8.0 and python3.9.7 and you might run into problems when using different versions, which is why we recommend installing snakemake as follows:\n\nmamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "source/installation_instructions.html#install-nanoits",
    "href": "source/installation_instructions.html#install-nanoits",
    "title": "Installation",
    "section": "",
    "text": "NanoClass can be installed via git as follows:\n\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\nIf you don’t have or want to install git, you can also download NanoClass as follows:\n\nGo to https://github.com/ndombrowski/NanoClass2\nClick the green code button\nDownload zip\nExtract zip"
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "NanoClass2",
    "section": "",
    "text": "NanoClass2 is a taxonomic meta-classifier for 16S/18S amplicon sequencing data generated with the Oxford Nanopore MinION. The first iteration of this workflow, NanoClass, was originally developed by Evelien Jongepier and the original version can be found here.\nWith a single command, you can run upto 11 classification tools on multiple samples in parallel, including BLASTN, Centrifuge, Kraken2, IDTAXA, MegaBLAST, dcMegaBLAST, Minimap2, Mothur, QIIME2, RDP and SPINGO. Read preparation steps, such as quality trimming, length filtering and sub-sampling, are an integral part of the pipeline.\nNanoClass automatically installs all software packages and dependencies, downloads and builds required taxonomic databases and runs the analysis on your samples.\n\n\nThe following changes were made compared to the first version of NanoClass:\n\nUse of SILVA 138.1_SSURef_NR99 instead pf SILVA 132_SSURef_NR99\nUpdating to Silva 138 now also means eukaryotic sequences are included in the database\nUpdated the Kraken2 version to work with the changed SILVA ftp path\nRead filtering is now done with Chopper instead of Nanofilt\nChopper allows additional quality filtering and to trim nucleotides from the beginning and the end of the read\nAll classifiers use now exactly the same reference files\nUpdated the output folder structure\nAdded additional parameters for kraken2 and minimap2\nClarified version numbers throughout the conda environment files"
  },
  {
    "objectID": "source/run_nanoclass.html",
    "href": "source/run_nanoclass.html",
    "title": "Running NanoClass2",
    "section": "",
    "text": "NanoClass2 will take de-multiplexed and compressed fastq files as input and generate OTU tables and several summary statistics as output.\nTo run NanoClass2 please, provide a single fastq.gz file per sample, i.e. concatenate your files before running NanoClass2.\nIf you have no preference for a classifier, you first might want to test, which classifier performs best on your data. You can do this by first subsampling your dataset and running all 11 classification tools on this smaller subsample. Tests on data sets of variable complexity have shown that a small subset of data is enough to assess tool performance. Once you have decided which tool or tools to use you can run this tool on your entire dataset.\n\n\nIn the configuration file, also called config.yaml, you can specify how NanoClass2 should be run.\nTo tell Snakemake where your data is located and to change the default settings start by copying the config.yaml file found in the NanoClass2 folder to the folder in which you want to analyse your data, i.e. like this:\n\ncp &lt;path_to_NanoClass2_folder&gt;/config.yaml .\n\nYou can also run your analyses in the NanoClass2 folder you downloaded, but it is preferable to separate software from analyses.\nNext, open the config.yaml with an editor, such as nano. There are several things you can modify:\n1. The samples you want to analyse\nHere, you need to provide the path to a comma-separated mapping file that describes the samples you want to analyse in the section: samples_file: \"example_files/mapping.csv\".\nThis file needs to contain the following columns:\n\nrun: The run name of your sample. This is useful if you want to run NanoClass several times on the same data but with different settings. Results for each run will be generated in different folders based on the run name. You can use any label as long as it consists of only letters and numbers.\nsample: The names of your sample. This ID will be used to label all files created in subsequent steps. Your sample names should be unique and only contain letters and numbers. Do not use other symbols, such as spaces, dots or underscores in your sample names.\nbarcode: The barcode column should remain empty as NanoClass2 currently only accepts demultiplexed data.\npath: Absolute path to the fastq.gz files. The workflow accepts one file per barcode, so if you have more than one file merge these files first, for example using the cat command.\n\nAn example file could look like this (exchange  with the absolute path to your samples. Preferably don’t use the relative path unless you start your analysis relative to where the Snakefile is.):\nrun,sample,barcode,path\ndemo,R1A,,&lt;absolute_path&gt;/barcode01_merged.fastq.gz\ndemo,R2A,,&lt;absolute_path&gt;/barcode02_merged.fastq.gz\ndemo,R3A,,&lt;absolute_path&gt;/barcode03_merged.fastq.gz\n2. The classifiers you want to use\nYou can choose what classifiers you want to use in methods: [\"blastn\",\"centrifuge\",\"dcmegablast\",\"idtaxa\",\"megablast\",\"minimap\",\"mothur\",\"qiime\",\"rdp\",\"spingo\"]. If you want to first compare different classifiers you can use the full list if you just want to use one (or two, …) classifiers you could for example write: methods: [\"blastn\"]\n3. Subsampling our reads\n\n\n\n\n\n\nImportant\n\n\n\nIn this section, we can specify whether we would like to subset the number of reads and how many reads per sample you like to include. This allows us to run NanoClass2 in two modes:\n\nWith subsampling: Use this, if you want to compare several classifiers on a smaller subsample of your data. We can subsample our data, by setting skip: false in the subsample section of the yaml file. Additionally, we can specify how many reads we want to analyse per sample with samplesize: 100.\nWithout subsampling: Use this, if you already know what classifier you want to use. You can toggle the subsampling off by setting using skip: true in the subsample section of the yaml file.\n\n\n\nIn the example below subsample is enabled and 100 random reads per sample will be included in the analyses.\nsubsample:\n    skip:                          false\n    samplesize:                    100\n    environment:                   \"preprocess.yml\"\nIn the second example below, we disable subsampling and analyse all our reads.\nsubsample:\n    skip:                          true\n    samplesize:                    100\n    environment:                   \"preprocess.yml\"\n4. Output folder name\nThe name of the folder in which the results should be stored in.\n5. Other settings\nFinally, you can change tool specific parameters: If desired, there are several parameters that can be changed by the user, such as the numbers of threads to use for the different tools or the settings used for the read quality filtering (in qual_filtering). Please check out the config.yaml file, which provides more detailed explanations on each setting.\n\n\n\n\n\nTo test whether the workflow is defined properly do a dry-run first. In the command below, do the following changes:\n\nProvide the path to where you installed NanoClass2 (and where the file specifying how the workflow is run, the Snakemake file, can be found) afer --s\nProvide the path to the edited config file after --configfile\nProvide the path to where you want Snakemake to install all program dependencies after --conda-prefix. We recommend to install these into the folder in which you downloaded NanoClass2 but you can change this if desired\n\n\n#activate conda environment with your snakemake installation, i.e. \nconda activate snakemake_nanoclass\n\n#perform a dry-run\nsnakemake --cores 1 \\\n    -s &lt;path_to_NanoClass2_install&gt;/Snakefile \\\n    --configfile &lt;path_to_edited_config_file&gt;/config.yaml \\\n    --use-conda --conda-prefix &lt;path_to_NanoClass2_install&gt;/.snakemake/conda \\\n    --nolock --rerun-incomplete -np\n\n\n\n\nIf the dry-run was successful you can run Snakemake interactively with the command below. Adjust the cores according to your system.\n\n\n\n\n\n\nWarning\n\n\n\nWhen running NanoClass2 for the first time, you might get a warning to set condas channel priority to strict when installing some dependencies, for example QIIME 2. If this happens we for now recommend setting the conda channel priorities to flexible to not run into issue when installing software dependencies.\nYou can check your channel priorities in ~/.condarc (Linux-environment)\nYou can reset the channel priorities with conda config --set channel_priority flexible.\n\n\n\nsnakemake --cores 1 \\\n    -s &lt;path_to_NanoClass2_install&gt;/Snakefile \\\n    --configfile config.yaml \\\n    --use-conda --conda-prefix &lt;path_to_NanoClass2_install&gt;/.snakemake/conda \\\n    --nolock --rerun-incomplete\n\n\n\n\nAfter a successful run, you can create a report with some of the key output files as follows:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s &lt;path_to_NanoClass2_install&gt;/Snakefile\n\nAfterwards, the report can be found in the output folder and includes:\n\nA shematic of the steps executed\nA PDF containing information about the reads after quality filtering. To view the full report, download the individual files as only the first page is shown in the preview\nBargraphs for different taxonomic ranks (and classifiers if more than one was used)\nInformation about the run-times for each sample and statistics on the runtimes for each tool/sample\n\nAdditionally, the output folder contains:\n\n&lt;output_dir&gt;/data/&lt;run&gt;/chopper: The filtered reads (useful if you want to do something else with these reads)\n&lt;output_dir&gt;/classifications/&lt;run&gt;/&lt;method&gt;: The OTU matrix for each sample and classifier\n&lt;output_dir&gt;/tables: An OTU as well as a taxonomy table combined for each sample and classifier used",
    "crumbs": [
      "Running NanoClass2"
    ]
  },
  {
    "objectID": "source/run_nanoclass.html#running-nanoclass2-1",
    "href": "source/run_nanoclass.html#running-nanoclass2-1",
    "title": "Running NanoClass2",
    "section": "Running NanoClass2",
    "text": "Running NanoClass2\n\nDry-run\nTo test whether the workflow is defined properly do a dry-run first. In the command below, do the following changes:\n\nProvide the path to where you installed NanoClass2 (and where the file specifying how the workflow is run, the Snakemake file, can be found) afer --s\nProvide the path to the edited config file after --configfile\nProvide the path to where you want Snakemake to install all program dependencies after --conda-prefix. We recommend to install these into the folder in which you downloaded NanoClass2 but you can change this if desired\n\n\n#activate conda environment with your snakemake installation, i.e. \nconda activate snakemake_nanoclass\n\n#perform a dry-run\nsnakemake --cores 1 \\\n    -s &lt;path_to_NanoClass2_install&gt;/Snakefile \\\n    --configfile &lt;path_to_edited_config_file&gt;/config.yaml \\\n    --use-conda --conda-prefix &lt;path_to_NanoClass2_install&gt;/.snakemake/conda \\\n    --nolock --rerun-incomplete -np\n\n\n\nRun NanoClass2 interactively\nIf the dry-run was successful you can run Snakemake interactively with the command below. Adjust the cores according to your system.\n\n\n\n\n\n\nWarning\n\n\n\nWhen running NanoClass2 for the first time, you might get a warning to set condas channel priority to strict when installing some dependencies, for example QIIME 2. If this happens we for now recommend setting the conda channel priorities to flexible to not run into issue when installing software dependencies.\nYou can check your channel priorities in ~/.condarc (Linux-environment)\nYou can reset the channel priorities with conda config --set channel_priority flexible.\n\n\n\nsnakemake --cores 1 \\\n    -s &lt;path_to_NanoClass2_install&gt;/Snakefile \\\n    --configfile config.yaml \\\n    --use-conda --conda-prefix &lt;path_to_NanoClass2_install&gt;/.snakemake/conda \\\n    --nolock --rerun-incomplete\n\n\n\nGenerate a report\nAfter a successful run, you can create a report with some of the key output files as follows:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s &lt;path_to_NanoClass2_install&gt;/Snakefile\n\nAfterwards, the report can be found in the output folder and includes:\n\nA shematic of the steps executed\nA PDF containing information about the reads after quality filtering. To view the full report, download the individual files as only the first page is shown in the preview\nBargraphs for different taxonomic ranks (and classifiers if more than one was used)\nInformation about the run-times for each sample and statistics on the runtimes for each tool/sample\n\nAdditionally, the output folder contains:\n\n&lt;output_dir&gt;/data/&lt;run&gt;/chopper: The filtered reads (useful if you want to do something else with these reads)\n&lt;output_dir&gt;/classifications/&lt;run&gt;/&lt;method&gt;: The OTU matrix for each sample and classifier\n&lt;output_dir&gt;/tables: An OTU as well as a taxonomy table combined for each sample and classifier used",
    "crumbs": [
      "Running NanoClass2"
    ]
  },
  {
    "objectID": "source/run_nanoclass.html#edit-the-configuration-file",
    "href": "source/run_nanoclass.html#edit-the-configuration-file",
    "title": "Running NanoClass2",
    "section": "Edit the configuration file",
    "text": "Edit the configuration file\nIn the configuration file, also called config.yaml, you can specify how NanoClass2 should be run.\nTo tell Snakemake where your data is located and to change the default settings start by copying the config.yaml file found in the NanoClass2 folder to the folder in which you want to analyse your data, i.e. like this:\n\ncp &lt;path_to_NanoClass2_folder&gt;/config.yaml .\n\nYou can also run your analyses in the NanoClass2 folder you downloaded, but it is preferable to separate software from analyses.\nNext, open the config.yaml with an editor, such as nano. There are several things you can modify:\n1. The samples you want to analyse\nHere, you need to provide the path to a comma-separated mapping file that describes the samples you want to analyse in the section: samples_file: \"example_files/mapping.csv\".\nThis file needs to contain the following columns:\n\nrun: The run name of your sample. This is useful if you want to run NanoClass several times on the same data but with different settings. Results for each run will be generated in different folders based on the run name. You can use any label as long as it consists of only letters and numbers.\nsample: The names of your sample. This ID will be used to label all files created in subsequent steps. Your sample names should be unique and only contain letters and numbers. Do not use other symbols, such as spaces, dots or underscores in your sample names.\nbarcode: The barcode column should remain empty as NanoClass2 currently only accepts demultiplexed data.\npath: Absolute path to the fastq.gz files. The workflow accepts one file per barcode, so if you have more than one file merge these files first, for example using the cat command.\n\nAn example file could look like this (exchange  with the absolute path to your samples. Preferably don’t use the relative path unless you start your analysis relative to where the Snakefile is.):\nrun,sample,barcode,path\ndemo,R1A,,&lt;absolute_path&gt;/barcode01_merged.fastq.gz\ndemo,R2A,,&lt;absolute_path&gt;/barcode02_merged.fastq.gz\ndemo,R3A,,&lt;absolute_path&gt;/barcode03_merged.fastq.gz\n2. The classifiers you want to use\nYou can choose what classifiers you want to use in methods: [\"blastn\",\"centrifuge\",\"dcmegablast\",\"idtaxa\",\"megablast\",\"minimap\",\"mothur\",\"qiime\",\"rdp\",\"spingo\"]. If you want to first compare different classifiers you can use the full list if you just want to use one (or two, …) classifiers you could for example write: methods: [\"blastn\"]\n3. Subsampling our reads\n\n\n\n\n\n\nImportant\n\n\n\nIn this section, we can specify whether we would like to subset the number of reads and how many reads per sample you like to include. This allows us to run NanoClass2 in two modes:\n\nWith subsampling: Use this, if you want to compare several classifiers on a smaller subsample of your data. We can subsample our data, by setting skip: false in the subsample section of the yaml file. Additionally, we can specify how many reads we want to analyse per sample with samplesize: 100.\nWithout subsampling: Use this, if you already know what classifier you want to use. You can toggle the subsampling off by setting using skip: true in the subsample section of the yaml file.\n\n\n\nIn the example below subsample is enabled and 100 random reads per sample will be included in the analyses.\nsubsample:\n    skip:                          false\n    samplesize:                    100\n    environment:                   \"preprocess.yml\"\nIn the second example below, we disable subsampling and analyse all our reads.\nsubsample:\n    skip:                          true\n    samplesize:                    100\n    environment:                   \"preprocess.yml\"\n4. Output folder name\nThe name of the folder in which the results should be stored in.\n5. Other settings\nFinally, you can change tool specific parameters: If desired, there are several parameters that can be changed by the user, such as the numbers of threads to use for the different tools or the settings used for the read quality filtering (in qual_filtering). Please check out the config.yaml file, which provides more detailed explanations on each setting.",
    "crumbs": [
      "Running NanoClass2"
    ]
  },
  {
    "objectID": "source/references.html",
    "href": "source/references.html",
    "title": "References",
    "section": "",
    "text": "NanoClass2 makes use of third-party software and resources. If you make use of NanoClass2 we recommend that you cite this software as well.\nThe version numbers are the exact versions used to develop this workflow.\n\n\n\nSILVA_138.1_SSURef_NR99 (Quast et al. 2013)\n\n\n\n\n\nSnakemake v6.8.0 (Mölder et al. 2021)\nPython v3.9.7\n\nnumpy v1.26.2\ntabulate v0.8\nbiopython==1.78\n\nR v4.0\n\nr-rlang v1.0.5\nr-base v4.0.5\nr-essentials v1.7.0\nphyloseq v1.34 (McMurdie and Holmes 2013)\ndada2 v1.18.0\nseqinr v4.2_16\ndecipher v2.18.1\nr-vroom v1.5.7\n\nPorechop v0.2.4, link to github\nChopper v0.6.0 (De Coster and Rademakers 2023)\nNanoStat v1.4.0 (De Coster et al. 2018)\nseqtk v1.3, link to github\nPistis v0.3.3, link to github\nblast v2.10.1 (Altschul et al. 1990)\nfasta-splitter v0.2.6 link to manual\nminimap2 v2.17 (Li 2018)\nsamtools v1.10 (Li et al. 2009)\ncentrifuge v1.0.4_beta (Kim et al. 2016)\nmothur v1.44 (Schloss et al. 2009)\nkraken2 v2.0.8_beta (Wood, Lu, and Langmead 2019)\nqiime2 v2020.8 (Estaki et al. 2020)\nspingo=1.3 (Allard et al. 2015)",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/code_dev.html#dry-run-getting-a-tax-map",
    "href": "source/code_dev.html#dry-run-getting-a-tax-map",
    "title": "NanoClass2",
    "section": "Dry run getting a tax map",
    "text": "Dry run getting a tax map\nThe issue is that\n\nthere is no file existing that was used for qiime in 132 so we have to do this from scratch\nthe files in common do not include eukaryotes and including euks is not so easy because the silva taxonomy for euks does not use an easy to parse taxonomy sheme (we have different elements for different taxa)\n\nFor an alternative way of parsing, we can later check:\nhttps://mothur.org/blog/2020/SILVA-v138-reference-files/\n\n#used vs: 0.15.0\nconda activate taxonkit\n\nwget https://raw.githubusercontent.com/DerrickWood/kraken2/master/scripts/build_silva_taxonomy.pl\n\n#prep folders \nmkdir -p db/common/data db/common/taxonomy db/common/library\n\n#download data\nwget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\nwget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/taxonomy/tax_slv_ssu_138.1.txt.gz\nwget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/taxonomy/tax_slv_ssu_138.1.acc_taxid.gz\n\ngzip -d db/common/*gz\n\n#remove spaces in the tax mapping to resolve issue with taxonkit:\n#sed -i \"s/ /_/g\" data/tax_slv_ssu_138.1.txt\n\n#get a taxonomy file\nperl build_silva_taxonomy.pl db/common/tax_slv_ssu_138.1.txt\n\nmv names.dmp nodes.dmp db/common/taxonomy\nmv db/common/SILVA_138.1_SSURef_NR99_tax_silva.fasta db/common/tax_slv_ssu_138.1.txt db/common/data \nmv  db/common/tax_slv_ssu_138.1.acc_taxid db/common/seqid2taxid.map\n\n#clean U \nsed -e '/^&gt;/!y/U/T/' db/common/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta &gt; db/common/ref-seqs.fna\n\ntaxonkit lineage --data-dir db/common/taxonomy &lt;(awk '{print $1}' db/common/taxonomy/names.dmp | sort | uniq) | \\\n taxonkit reformat -r NA -a -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/common/taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; db/common/taxid_to_tax.txt\n\nLC_ALL=C join -1 2 -2 1 -t $'\\t' &lt;(LC_ALL=C sort -k 2 db/common/seqid2taxid.map) &lt;(LC_ALL=C sort -k1 db/common/taxid_to_tax.txt) \\\n            | awk -F \"\\t\" -v OFS=\"\\t\" '{{print $2, $3}}' &gt; test.txt\n\n\nconda activate /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/a8242208738e7b5509d605409fa6d659\n\nsrun -n 1 --cpus-per-task 8 kraken2-build --db db/kraken --special silva --threads 8\n\n\nTest-run\n\nSetup\n\nwdir=\"/home/ndombro/personal/testing\"\ncd $wdir\n\n#setup snakemake env\n#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\nconda activate snakemake_nanoclass\n\n\n\nPrep config\n\nmkdir input\n\n#get config \ncp ~/personal/snakemake_workflows/NanoClass2/config.yaml .\n\n#cp batch script\ncp ~/personal/snakemake_workflows/NanoClass2/jobscript.sh .\n\n#get mapping file\ncp ~/personal/snakemake_workflows/NanoClass2/example_files/mapping.csv .\n\nEdits:\n\nconfig.yaml:\n\nChange path to mapping.csv to mapping.csv\n\nChanges to mapping.csv:\n\nChange $PWD to /home/ndombro//personal/snakemake_workflows/NanoClass2/example_files/\n\njobscript.sh\n\nChange line with snakmake command to cmd=\"srun --cores $SLURM_CPUS_ON_NODE  snakemake -s ~/personal/snakemake_workflows/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix ~/personal/snakemake_workflows/NanoClass2/.snakemake/conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete\"\n\n\n\n#dry run\nsnakemake -s ~/personal/snakemake_workflows/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix ~/personal/snakemake_workflows/NanoClass2/.snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\nGet report:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s ~/personal/snakemake_workflows/NanoClass2/Snakefile\n\nNotice:\n\npreprocess runs fine with Python 3.10.13"
  },
  {
    "objectID": "source/references.html#database",
    "href": "source/references.html#database",
    "title": "References",
    "section": "Database",
    "text": "Database\n\nSILVA_138.1_SSURef_NR99 (Quast et al. 2013)",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/references.html#software",
    "href": "source/references.html#software",
    "title": "References",
    "section": "Software",
    "text": "Software\n\nSnakemake v6.8.0 (Mölder et al. 2021)\nPython v3.9.7\n\nnumpy v1.26.2\ntabulate v0.8\nbiopython==1.78\n\nR v4.0\n\nr-rlang v1.0.5\nr-base v4.0.5\nr-essentials v1.7.0\nphyloseq v1.34 (McMurdie and Holmes 2013)\ndada2 v1.18.0\nseqinr v4.2_16\ndecipher v2.18.1\nr-vroom v1.5.7\n\nPorechop v0.2.4, link to github\nChopper v0.6.0 (De Coster and Rademakers 2023)\nNanoStat v1.4.0 (De Coster et al. 2018)\nseqtk v1.3, link to github\nPistis v0.3.3, link to github\nblast v2.10.1 (Altschul et al. 1990)\nfasta-splitter v0.2.6 link to manual\nminimap2 v2.17 (Li 2018)\nsamtools v1.10 (Li et al. 2009)\ncentrifuge v1.0.4_beta (Kim et al. 2016)\nmothur v1.44 (Schloss et al. 2009)\nkraken2 v2.0.8_beta (Wood, Lu, and Langmead 2019)\nqiime2 v2020.8 (Estaki et al. 2020)\nspingo=1.3 (Allard et al. 2015)",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/installation_instructions.html#install-nanoclass2",
    "href": "source/installation_instructions.html#install-nanoclass2",
    "title": "Installation",
    "section": "Install NanoClass2",
    "text": "Install NanoClass2\nNanoClass2 can be installed via git as follows:\n\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\nIf you don’t have or want to install git, you can also download NanoClass as follows:\n\nGo to https://github.com/ndombrowski/NanoClass2\nClick the green code button\nDownload zip\nExtract zip",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "source/other_dbs.html",
    "href": "source/other_dbs.html",
    "title": "Other databases",
    "section": "",
    "text": "NanoClass was designed to classify 16S or 18S rRNA gene amplicon sequences based on the SILVA database. Most tools implemented in NanoClass2 will be able to classify sequences using alternative databases. The use of custom databases has not been fully tested and comes with no warranties, but some advice on how to prepare alternative databases, such as Unite, can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nTo reduce runtime, NanoClass2 will only run those steps in the pipeline for which no output files are detected. This means that if a database was downloaded and build during an earlier run of NanoClass2, it will automatically use this database instead of downloading and building a new one. This saves a lot of time and computational resources, but also requires care by the user to make sure the intended database is used.\nIt is recommended to use a fresh NanoClass2 application if a different database is used. Simply clone or download NanoClass2 again following the instructions in the quick start section of this documentation.\n\n\n\n\nMost classification tools implemented in NanoClass2 can use alternative databases supplied by the user (see exceptions below), provided they are formatted correctly. These databases will not be automatically downloaded as part of the NanoClass pipeline, as is the case for the SILVA 16S or 18S databases. Instead, users should provide the databases themselves and store them in the db/common/ subdirectory of the NanoClass2 directory. These will then be automatically detected once NanoClass2 is started and NanoClass2 will create and reformat all tools-specific databases based on this user-provided database.\nTwo files are required:\nThe reference sequences\nThe reference nucleotide sequences should be in an unzipped, multi-fasta file called ref-seqs.fna. The fasta header should have the sequence identifier only.\nHere is an example of an entry in the ref-seqs.fna of the default SILVA reference database:\n&gt;AY846379.1.1791\nAACCTGGTTGATCCTGCCAGTAGTCATATGCTTGTCTCAAAGATTAAGCCATGCATGTCTAAGTATAAACTGCTTATACT\nGTGAAACTGCGAATGGCTCATTAAATCAGTTATAGTTTATTTGATGGTACCTCTACACGGATAACCGTAGTAATTCTAGA\nThe reference taxonomy\nThe reference taxonomy should be in an unzipped tab-delimited file called ref-taxonomy.txt. The first column contains the sequence identifier corresponding to the identifiers used in the ref-seqs.fna. The second column contains a “;”-separated 7-level taxonomy string with domain;phylum;class;order;family;genus;species.\nHere is an example of some of the entries in ref-taxonomy.txt of the default SILVA reference database.\nLT595716.1.1435 D_0__Bacteria;D_1__Firmicutes;D_2__Bacilli;D_3__Lactobacillales;D_4__Carnobacteriaceae;D_5__Jeotgalibaca;D_6__NA\nAB696431.1.1480 D_0__Bacteria;D_1__Firmicutes;D_2__Bacilli;D_3__uncultured;D_4__NA;D_5__NA;D_6__NA\nAACY020207233.1273.2814 D_0__Bacteria;D_1__Marinimicrobia (SAR406 clade);D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA\n\n\n\nAlthough not automatically implemented in NanoClass2, it is possible to manually download the ITS UNITE database to be used in NanoClass2. Note, that this might only work for a subset of databases.\nFirst download the UNITE database from here. Then unzip it and look for the reference sequences (sh_refs_qiime_ver8_99_04.02.2020.fasta) and the reference taxonomy (sh_taxonomy_qiime_ver8_99_04.02.2020.txt).\nMove these two files to the to a db/common/ directory in the NanoClass2 folder and rename them to ref-seqs.fna and ref-taxonomy.txt, respectively.\n\nmkdir -p db/common/\nmkdir -p db/common/taxonomy\nmkdir -p db/common/data\n\nmv sh_refs_qiime_ver8_99_04.02.2020.fasta db/common/ref-seqs.fna\nmv sh_taxonomy_qiime_ver8_99_04.02.2020.txt db/common/ref-taxonomy.txt\n\nLastly, you need to create some empty files.\n\n#dummy file for mothur\ntouch db/common/ref-seqs.aln\n\n#dummy files for kraken and centrifuge\ntouch db/common/seqid2taxid.map\ntouch db/common/taxonomy/names.dmp\ntouch db/common/taxonomy/nodes.dmp\ntouch db/common/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta\ntouch db/common/ref-seqs-tax.fna\n\nThese empty files are expected by some classifiers and you might run into issues when not creating these dummy files first.\nFor now, these steps should allow you to use the following classifiers: blastn, dcmegablast, idtaxa, megablast, minimap, qiime, rdp and spingo.\n\n\n\nMothur\nFor Mothur an additional fasta file is required with the multiple sequence alignments of all reference sequences in the database. The fasta header should be the same as the fasta header of the sequence file. The file should be named ref-seqs.aln and stored in db/common in the NanoClass2 folder.\nIf you do not have an alignment for your custom database, you can simply skip mother in the NanoClass2 run by removing it from the methods list in the config.yaml\nKraken2\nKraken2 cannot be used within NanoClass in combination with custom databases at this stage.\nCentrifuge\nCentrifuge cannot be used within NanoClass in combination with custom databases at this stage.",
    "crumbs": [
      "Other databases"
    ]
  },
  {
    "objectID": "source/other_dbs.html#custom-databases",
    "href": "source/other_dbs.html#custom-databases",
    "title": "Other databases",
    "section": "Custom databases",
    "text": "Custom databases\nMost classification tools implemented in NanoClass2 can use alternative databases supplied by the user (see exceptions below), provided they are formatted correctly. These databases will not be automatically downloaded as part of the NanoClass pipeline, as is the case for the SILVA 16S or 18S databases. Instead, users should provide the databases themselves and store them in the db/common/ subdirectory of the NanoClass2 directory. These will then be automatically detected once NanoClass2 is started and NanoClass2 will create and reformat all tools-specific databases based on this user-provided database.\nTwo files are required:\nThe reference sequences\nThe reference nucleotide sequences should be in an unzipped, multi-fasta file called ref-seqs.fna. The fasta header should have the sequence identifier only.\nHere is an example of an entry in the ref-seqs.fna of the default SILVA reference database:\n&gt;AY846379.1.1791\nAACCTGGTTGATCCTGCCAGTAGTCATATGCTTGTCTCAAAGATTAAGCCATGCATGTCTAAGTATAAACTGCTTATACT\nGTGAAACTGCGAATGGCTCATTAAATCAGTTATAGTTTATTTGATGGTACCTCTACACGGATAACCGTAGTAATTCTAGA\nThe reference taxonomy\nThe reference taxonomy should be in an unzipped tab-delimited file called ref-taxonomy.txt. The first column contains the sequence identifier corresponding to the identifiers used in the ref-seqs.fna. The second column contains a “;”-separated 7-level taxonomy string with domain;phylum;class;order;family;genus;species.\nHere is an example of some of the entries in ref-taxonomy.txt of the default SILVA reference database.\nLT595716.1.1435 D_0__Bacteria;D_1__Firmicutes;D_2__Bacilli;D_3__Lactobacillales;D_4__Carnobacteriaceae;D_5__Jeotgalibaca;D_6__NA\nAB696431.1.1480 D_0__Bacteria;D_1__Firmicutes;D_2__Bacilli;D_3__uncultured;D_4__NA;D_5__NA;D_6__NA\nAACY020207233.1273.2814 D_0__Bacteria;D_1__Marinimicrobia (SAR406 clade);D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA",
    "crumbs": [
      "Other databases"
    ]
  },
  {
    "objectID": "source/other_dbs.html#example-using-unite",
    "href": "source/other_dbs.html#example-using-unite",
    "title": "Other databases",
    "section": "Example using Unite",
    "text": "Example using Unite\nAlthough not automatically implemented in NanoClass2, it is possible to manually download the ITS UNITE database to be used in NanoClass2. Note, that this might only work for a subset of databases.\nFirst download the UNITE database from here. Then unzip it and look for the reference sequences (sh_refs_qiime_ver8_99_04.02.2020.fasta) and the reference taxonomy (sh_taxonomy_qiime_ver8_99_04.02.2020.txt).\nMove these two files to the to a db/common/ directory in the NanoClass2 folder and rename them to ref-seqs.fna and ref-taxonomy.txt, respectively.\n\nmkdir -p db/common/\nmkdir -p db/common/taxonomy\nmkdir -p db/common/data\n\nmv sh_refs_qiime_ver8_99_04.02.2020.fasta db/common/ref-seqs.fna\nmv sh_taxonomy_qiime_ver8_99_04.02.2020.txt db/common/ref-taxonomy.txt\n\nLastly, you need to create some empty files.\n\n#dummy file for mothur\ntouch db/common/ref-seqs.aln\n\n#dummy files for kraken and centrifuge\ntouch db/common/seqid2taxid.map\ntouch db/common/taxonomy/names.dmp\ntouch db/common/taxonomy/nodes.dmp\ntouch db/common/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta\ntouch db/common/ref-seqs-tax.fna\n\nThese empty files are expected by some classifiers and you might run into issues when not creating these dummy files first.\nFor now, these steps should allow you to use the following classifiers: blastn, dcmegablast, idtaxa, megablast, minimap, qiime, rdp and spingo.",
    "crumbs": [
      "Other databases"
    ]
  },
  {
    "objectID": "source/other_dbs.html#exceptions",
    "href": "source/other_dbs.html#exceptions",
    "title": "Other databases",
    "section": "Exceptions",
    "text": "Exceptions\nMothur\nFor Mothur an additional fasta file is required with the multiple sequence alignments of all reference sequences in the database. The fasta header should be the same as the fasta header of the sequence file. The file should be named ref-seqs.aln and stored in db/common in the NanoClass2 folder.\nIf you do not have an alignment for your custom database, you can simply skip mother in the NanoClass2 run by removing it from the methods list in the config.yaml\nKraken2\nKraken2 cannot be used within NanoClass in combination with custom databases at this stage.\nCentrifuge\nCentrifuge cannot be used within NanoClass in combination with custom databases at this stage.",
    "crumbs": [
      "Other databases"
    ]
  }
]