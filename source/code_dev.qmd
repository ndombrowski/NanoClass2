---
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left

  pdf:
    documentclass: scrreprt
    toc: true
    toc-depth: 4
    #pdf-engine: pdflatex
    fig-width: 3.5
    fig-height: 3.5
    geometry:
      - top=25mm
      - bottom=20mm
      - heightrounded
    highlight-style: github
    pandoc_args: --listings
    header-includes:
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
        \DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
    colorlinks: true
    code-block-bg: D3D3D3

execute:
  eval: false

engine: knitr
---


## Notes on code development

Vroom version: vroom_1.5.7


kraken2 --db /home/ndombro/personal/testing/NanoClass2/./db/common             --confidence 0.0             --output classifications/demo/kraken/R1A.kraken.out             --report classifications/demo/kraken/R1A.kraken.report             --gzip-compressed             --threads 16 data/demo/chopper/R1A.subsampled.fastq.gz

Github testrun:

```{bash}
cd /home/ndombro/personal/testing

git clone https://github.com/ndombrowski/NanoClass2.git

mkdir analyses
cd analyses

cp ~/personal/testing/NanoClass2/config.yaml .
cp ~/personal/testing/NanoClass2/jobscript.sh .

snakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix /home/ndombro/personal/testing/NanoClass2/.snakemake/conda --cores 1 --nolock --rerun-incomplete -np

snakemake -s /home/ndombro/personal/testing/NanoClass2/Snakefile --configfile config.yaml --report
```


Changes made to config.yaml:

- samples:                           "../NanoClass2/example_files/mapping.csv"

Changes to mapping.csv

- Change path to: `/home/ndombro/personal/testing/NanoClass2/example_files/barcode01_merged.fastq.gz`


 /home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -k classifications/demo/kraken/R3A.kraken.out -f /home/ndombro/personal/testing/NanoClass2/./db/kraken/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta           -m /home/ndombro/personal/testing/NanoClass2/./db/kraken/seqid2taxid.map






### Sanity checks

#### Controlling otu table counts

Centrifuge has a problem that the counts are larger than 100. To solve this, we set the -k to 1 for now. For later

```{bash}
cd results/classifications/demo/centrifuge

#check why we do not max out at 100
for file in */*otumat; do
    sum=$(awk -F'\t' 'NR>1 {sum+=$2} END {print sum}' "$file")
    echo "Sum of Column 2 in $file: $sum"
done

#check unique labels in the raw reads (are 100)
for file in *out; do
    count=$(awk  -F'\t' '{print $1}' "$file" | sort | uniq | wc -l)
    echo "read number in $file: $count"
done

#check total counts
wc -l *out

#check unique genera in the raw reads (are 100)
for file in *out; do
    count=$(awk -F'\t' '{print $2}' "$file" | sort | uniq | wc -l)
    echo "read number in $file: $count"
done


#get a new ref file
python add_taxonomy.py -i ref-seqs.fna -t ref-taxonomy.txt -o ref-seqs-tax.fna



conda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/e3443d8e60db4c8046194cb2448abc78

python3 /home/ndombro/personal/testing/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R1A.centrifuge.out -f /home/ndombro/personal/testing/NanoClass2/db/common/ref-seqs-tax.fna


```


Sum of Column 2 in centrifuge/R1A.centrifuge.otumat: 84
Sum of Column 2 in centrifuge/R2A.centrifuge.otumat: 74
Sum of Column 2 in centrifuge/R3A.centrifuge.otumat: 110




Problem with R:

classifications/demo/kraken/R1A.kraken.taxmat, classifications/demo/kraken/R2A.kraken.taxmat, classifications/demo/kraken/R3A.kraken.taxmat

```{bash}
conda activate /home/ndombro/personal/testing/NanoClass2/.snakemake/conda/fbab651501bcaffe2471e33eec5a7215

Rscript /home/ndombro/personal/testing/NanoClass2//scripts/barplot.R classifications/demo/kraken/R1A.kraken.taxmat classifications/demo/kraken/R2A.kraken.taxmat classifications/demo/kraken/R3A.kraken.taxmat
```

Returns :

Rows: 152 Columns: 7
── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────
Delimiter: "\t"
chr (7): #taxid, Domain, Phylum, Class, Order, Family, Genus

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

```{bash}
suppressPackageStartupMessages(library(phyloseq))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(vroom))
suppressPackageStartupMessages(library(tidyr))

args <- c("classifications/demo/kraken/R1A.kraken.taxmat",
                "classifications/demo/kraken/R2A.kraken.taxmat", 
                "classifications/demo/kraken/R3A.kraken.taxmat")


taxmat <- as.data.frame(unique(vroom(delim = '\t', args)))
taxmat <- taxmat[!is.na(taxmat$Domain),]

taxmat <- unique(taxmat)
names(taxmat)[1] <- "taxid"
#$write.table(taxmat, file="tables/taxonomy-table.tsv", row.names=F, col.names=T, sep='\t', quote=F)

rownames(taxmat) <- taxmat$taxid
taxmat$taxid <- NULL
taxmat <- as.matrix(taxmat)

file <- gsub(".taxmat$", ".otumat", args)
sam <- data.frame(run = rep(NA, length(file)), sample = rep(NA, length(file)), method = rep(NA, length(file)))

for (i in 1:length(file)){
  otumatje <- read.table(file[i], header = T, sep = '\t', comment = "")
  names(otumatje)[1] <- "taxid"
  names(otumatje)[2] <- paste0(strsplit(file[i], "/")[[1]][2],"_",names(otumatje)[2])
  ifelse (i == 1, otumat <- otumatje, otumat <- merge(otumat, otumatje, all=TRUE))

  lab = strsplit(file[i], "/")[[1]][4]
  sam$run[i] = strsplit(file[i], "/")[[1]][2]
  sam$sample[i] = strsplit(lab, "[.]")[[1]][1]
  sam$method[i] = strsplit(lab, "[.]")[[1]][2]
} 

otumat[is.na(otumat)] <- 0

## for some custom DB like BOLD local copy, the same taxonomic lineage may occur 2x.
## This causes an error when they are used as rownames, which should be unique --> fix: aggregate
otumat <- aggregate(otumat[2:length(otumat)], by=list(otumat$taxid), sum)
names(otumat)[1] <- "taxid"
#write.table(otumat, file="tables/otu-table.tsv", row.names=F, col.names=T, sep='\t', quote=F)

rownames(otumat) <- otumat$taxid
otumat$taxid <- NULL
otumat <- as.matrix(otumat)

rownames(sam) <- colnames(otumat)


OTU = otu_table(otumat, taxa_are_rows = TRUE)
TAX = tax_table(taxmat)
SAM = sample_data(sam)

physeq = phyloseq(OTU, TAX, SAM)
pphyseq  = transform_sample_counts(physeq, function(x) x / sum(x) )

theme_set(theme_bw())

top.taxa <- tax_glom(physeq, "Class")
TopNOTUs <- names(sort(taxa_sums(top.taxa), TRUE)[1:20])
BottumNOTUs <- names(taxa_sums(top.taxa))[which(!names(taxa_sums(top.taxa)) %in% TopNOTUs)]
merged_physeq = merge_taxa(top.taxa, BottumNOTUs, 2)

mdf = psmelt(merged_physeq); names(mdf)[names(mdf) == "Class"] <- "level"
mdf$OTU[which(is.na(mdf$level))] <- "aaaOther"
mdf$level[which(is.na(mdf$level))] <- "aaaOther"
aggr_mdf <- aggregate(Abundance ~ sample + run + method + level, data = mdf, sum)

labs = aggr_mdf$level; labs[labs=="aaaOther"] <- "Other"
cols = scales::hue_pal()(length(unique(labs))); cols[unique(labs) == "Other"] <- "#CCCCCC"

p = ggplot(aggr_mdf, aes_string(x = "method", y = "Abundance", fill = "level"))
p = p + scale_fill_manual(name = "Phylum", labels = unique(labs), values = cols)
p = p + facet_grid(paste0("", aggr_mdf$run) ~ paste0("", aggr_mdf$sample))
p = p + geom_bar(stat = "identity", position = "stack",  color = "black", size = 0.1)
p = p + guides(fill=guide_legend(ncol=1))
p = p + labs(x = "Method", y = "Absolute abundance")
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))
p

p = ggplot(aggr_mdf, aes_string(x = "sample", y = "Abundance", fill = "level")) 
p = p + scale_fill_manual(name = level, labels = unique(labs), values = cols)
p = p + facet_grid(paste0("", aggr_mdf$run) ~ paste0("", aggr_mdf$method))
p = p + geom_bar(stat = "identity", position = "stack",  color = "black", size = 0.1)
p =	p + guides(fill=guide_legend(ncol=1))  
p = p + labs(x = "Sample", y = "Absolute abundance")
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 5))
p

```




To do:

- Make all classifiers work with new silva db
- check cutoffs for:
  - kraken
  - minimap

Kraken two downloads:

- SILVA_138.1_SSURef_NR99_tax_silva.fasta
-  tax_slv_ssu_138.1.txt


Tested classifiers:

- kraken, runs
- minimap, runs
- blastn, runs
- dcmegablast, runs
- megablast, runs
- rdp, running
- spingo, running
- idtaxa, running
- mothur, needs aln --> change common
- qiime, should work as is
- centrifuge, change mapurls


https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva_full_align_trunc.fasta.gz

https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_tax_silva_full_align_trunc.fasta.gz





Error in rule centrifuge_tomat:
    jobid: 39
    output: classifications/demo/centrifuge/R3A.centrifuge.taxlist, classifications/demo/centrifuge/R3A.centrifuge.taxmat, classifications/demo/centrifuge/R3A.centrifuge.otumat
    log: logs/demo/centrifuge_tomat_R3A.log (check log file(s) for error message)
    conda-env: /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9
    shell:
        /home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/c$        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

```{bash}
conda activate /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/c98e5bd467915505f1fa725bfa0a6cc9

/home/ndombro/personal/snakemake_workflows/NanoClass2/scripts/tomat.py -c results/classifications/demo/centrifuge/R3A.centrifuge.out -f /home/ndombro/personal/snakemake_workflows/NanoClass2/./db/common/ref-seqs.fna 


```






Centrifuge:

default path: https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz
default content:

AC200763        88363   89885   Bacteria;Proteobacteria

parsed default content:

AC200763.88363.89885    138073
AC201869.46386.47908    138073





```{bash}
#wget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1\".\"$2\".\"$3\"\t\"$(NF)}' > test_default.txt

wget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/taxonomy/taxmap_embl_ssu_ref_nr99_132.txt.gz -q -O - | gzip -d -c - | awk '{print $1"."$2"."$3"\t"$(NF)}' > test_default.txt


```


## Dry run getting a tax map

The issue is that 

1. there is no file existing that was used for qiime in 132 so we have to do this from scratch
2. the files in common do not include eukaryotes and including euks is not so easy because the silva taxonomy for euks does not use an easy to parse taxonomy sheme (we have different elements for different taxa) 

For an alternative way of parsing, we can later check:

https://mothur.org/blog/2020/SILVA-v138-reference-files/

```{bash}
#used vs: 0.15.0
conda activate taxonkit

wget https://raw.githubusercontent.com/DerrickWood/kraken2/master/scripts/build_silva_taxonomy.pl

#prep folders 
mkdir -p db/common/data db/common/taxonomy db/common/library

#download data
wget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz
wget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/taxonomy/tax_slv_ssu_138.1.txt.gz
wget -P db/common https://ftp.arb-silva.de/release_138.1/Exports/taxonomy/tax_slv_ssu_138.1.acc_taxid.gz

gzip -d db/common/*gz

#remove spaces in the tax mapping to resolve issue with taxonkit:
#sed -i "s/ /_/g" data/tax_slv_ssu_138.1.txt

#get a taxonomy file
perl build_silva_taxonomy.pl db/common/tax_slv_ssu_138.1.txt

mv names.dmp nodes.dmp db/common/taxonomy
mv db/common/SILVA_138.1_SSURef_NR99_tax_silva.fasta db/common/tax_slv_ssu_138.1.txt db/common/data 
mv  db/common/tax_slv_ssu_138.1.acc_taxid db/common/seqid2taxid.map

#clean U 
sed -e '/^>/!y/U/T/' db/common/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta > db/common/ref-seqs.fna

taxonkit lineage --data-dir db/common/taxonomy <(awk '{print $1}' db/common/taxonomy/names.dmp | sort | uniq) | \
 taxonkit reformat -r NA -a -f "{k}\t{p}\t{c}\t{o}\t{f}\t{g}\t{s}" --data-dir db/common/taxonomy | \
 awk 'BEGIN { FS = OFS = "\t" } {print $1,"D_0__"$3";D_1__"$4";D_2__"$5";D_3__"$6";D_4__"$7";D_5__"$8";D_6__"$9}' > db/common/taxid_to_tax.txt

LC_ALL=C join -1 2 -2 1 -t $'\t' <(LC_ALL=C sort -k 2 db/common/seqid2taxid.map) <(LC_ALL=C sort -k1 db/common/taxid_to_tax.txt) \
            | awk -F "\t" -v OFS="\t" '{{print $2, $3}}' > test.txt


```




```{bash}
conda activate /home/ndombro/personal/snakemake_workflows/NanoClass2/.snakemake/conda/a8242208738e7b5509d605409fa6d659

srun -n 1 --cpus-per-task 8 kraken2-build --db db/kraken --special silva --threads 8
```

### Test-run

#### Setup

```{bash}
wdir="/home/ndombro/personal/testing"
cd $wdir

#setup snakemake env
#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8

conda activate snakemake_nanoclass
```


#### Prep config

```{bash}
mkdir input

#get config 
cp ~/personal/snakemake_workflows/NanoClass2/config.yaml .

#cp batch script
cp ~/personal/snakemake_workflows/NanoClass2/jobscript.sh .

#get mapping file
cp ~/personal/snakemake_workflows/NanoClass2/example_files/mapping.csv .
```

Edits:

- config.yaml:
  - Change path to mapping.csv to `mapping.csv`
- Changes to mapping.csv:
  - Change `$PWD` to `/home/ndombro//personal/snakemake_workflows/NanoClass2/example_files/` 
- jobscript.sh
  - Change line with snakmake command to `cmd="srun --cores $SLURM_CPUS_ON_NODE  snakemake -s ~/personal/snakemake_workflows/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix ~/personal/snakemake_workflows/NanoClass2/.snakemake/conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete"`


```{bash}
#dry run
snakemake -s ~/personal/snakemake_workflows/NanoClass2/Snakefile --configfile config.yaml --use-conda --conda-prefix ~/personal/snakemake_workflows/NanoClass2/.snakemake/conda --cores 1 --nolock --rerun-incomplete -np
```

Get report:

```{bash}
snakemake --report report.html \
  --configfile config.yaml \
  -s ~/personal/snakemake_workflows/NanoClass2/Snakefile
```


Notice:

- preprocess runs fine with Python 3.10.13